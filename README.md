
# ğŸš€ Spaceship Titanic â€“ Machine Learning Classification Project

### ğŸ§  Overview
This project is based on the **Kaggle Spaceship Titanic Competition**, a futuristic twist on the classic Titanic dataset.  
The goal is to **predict whether a passenger was transported to an alternate dimension** after the spaceship Titanic encountered a spacetime anomaly.

Itâ€™s the year 2912 â€” and your mission as a Data Scientist is to help solve this interstellar mystery using **data-driven insights**.

---

### ğŸ¯ Objective
The main objective of this project is to build a **binary classification model** that predicts the â€œTransportedâ€ status (Yes/No) of each passenger.

---

### ğŸ“š Project Workflow
1. **Data Understanding** â€“ Loading and exploring the dataset to understand the structure and missing values.  
2. **Exploratory Data Analysis (EDA)** â€“ Visualizing passenger demographics, spending patterns, and transport outcomes.  
3. **Feature Engineering** â€“ Creating new meaningful features and encoding categorical data.  
4. **Data Preprocessing** â€“ Handling missing values, scaling numerical columns, and preparing data for modeling.  
5. **Model Building** â€“ Training and evaluating different machine learning models (Logistic Regression, Random Forest, XGBoost, etc.).  
6. **Hyperparameter Tuning** â€“ Optimizing model performance using GridSearchCV.  
7. **Prediction** â€“ Making final predictions on the test dataset using the best-performing model.

---

### ğŸ§© Tech Stack
- **Programming Language:** Python  
- **Libraries Used:**  
  - pandas  
  - numpy  
  - matplotlib  
  - seaborn  
  - scikit-learn  
  - xgboost

---

### ğŸ“Š Results & Insights
- The final model achieved high accuracy and provided key insights into the factors influencing passenger transportation.  
- Features like **CryoSleep**, **Cabin Location**, and **Spending on Amenities** had strong correlations with the outcome.

---

### ğŸ† Model Performance

| Model | Accuracy | F1 Score | Comments |
|-------|-----------|-----------|-----------|
| Logistic Regression | 0.79 | 0.78 | Baseline model |
| Random Forest | 0.83 | 0.82 | Good generalization |
| XGBoost | **0.85** | **0.84** | Best performing model |

---

### ğŸ’¡ Key Learnings
- Improved accuracy through feature engineering and parameter tuning.  
- Learned the importance of handling missing values carefully.  
- Understood the impact of categorical encoding and feature scaling on model performance.  
- Practiced model comparison and hyperparameter optimization techniques.  

---

### ğŸ“ Dataset
The dataset is sourced from the official Kaggle competition:  
ğŸ”— [Spaceship Titanic on Kaggle](https://www.kaggle.com/competitions/spaceship-titanic)

It contains columns like:  
- `PassengerId`, `HomePlanet`, `CryoSleep`, `Cabin`, `Destination`, `Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`, `Transported`.

---

### âš™ï¸ How to Run Locally
1. Clone this repository  
   ```bash
   git clone https://github.com/your-username/spaceship-titanic.git
   cd spaceship-titanic
   ```
2. Install required libraries  
   ```bash
   pip install -r requirements.txt
   ```
3. Open the Jupyter Notebook  
   ```bash
   jupyter notebook Spaceship_Titanic.ipynb
   ```
4. Run all cells to reproduce the results.

---

### ğŸ§‘â€ğŸ’» Author
**Fajlur Rahman**  
- Data Scientist | Machine Learning Enthusiast  
- ğŸ”— [LinkedIn](https://www.linkedin.com/in/fajlurrahman219)  
- ğŸ“§ fajlurrahman219@gmail.com

---

### â­ Contribute
Contributions, feedback, and suggestions are welcome! Feel free to fork this repo and raise a pull request.
